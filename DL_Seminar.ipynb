{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY5EHv_BwwuF",
        "outputId": "2383f0d5-e51f-4ccd-bdad-befea17272e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount to google drive to access training data"
      ],
      "metadata": {
        "id": "Y87IWsLW6cAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM51I1RYf1z7",
        "outputId": "bbb6fca2-96a8-47f4-c0e4-5ca805583c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "S7TwoJX-6Za3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import textwrap\n",
        "\n",
        "from IPython.display import display, clear_output\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn import Parameter\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "model_name = \"gpt2-medium\"\n",
        "\n",
        "# Using GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cZcmo6Axbet",
        "outputId": "02452801-852d-4915-c63c-38914ea2d3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Fine-tune dataset\n",
        "I got the data for this example from https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset?rvi=1"
      ],
      "metadata": {
        "id": "vqTK1BuMzRz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = \"<PAD>\"\n",
        "\n",
        "path_to_file = \"\" # path to file in google drive\n",
        "\n",
        "dataset = load_dataset('csv', data_files=path_to_file)['train'][\"text\"]\n",
        "\n",
        "print(\"{} Entries in dataset.\".format(len(dataset)))\n",
        "print(\"\\n\")\n",
        "print(\"Examples: \")\n",
        "for i in range(10):\n",
        "    print(dataset[i])"
      ],
      "metadata": {
        "id": "nF92jw9YzVvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ce4778-5fbb-4dea-ce94-ad314ad6bfca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000 Entries in dataset.\n",
            "\n",
            "\n",
            "Examples: \n",
            "TITLE: Why Do Fall Leaves Change Color? DESCRIPTION: Fall foliage delights leaf-peeping tourists, but how does the change in color benefit trees? As scientists explain, there is a reason for the season.\n",
            "TITLE: Falling Oil Hits Europe; Dollar Bounces DESCRIPTION:  LONDON (Reuters) - Most European stock markets followed  Wall Street lower Wednesday as crude oil's slide to three-month  lows hit heavily weighted oil shares, although a recovering  dollar was a plus for the region's exporters.\n",
            "TITLE: Linksys goes dual-band on Wi-Fi (MacCentral) DESCRIPTION: MacCentral - With its eyes on the future of home entertainment and a relatively uncluttered band of radio spectrum, Cisco Systems Inc.'s Linksys division on Wednesday unveiled a line of IEEE 802.11g/a wireless LAN products.\n",
            "TITLE: Chirac hits out at international community's inaction in Middle East (AFP) DESCRIPTION: AFP - French President Jacques Chirac sharply criticized the international community's failure to help put an end to the Israeli-Palestinian conflict, saying the world should \"impose\" peace talks.\n",
            "TITLE: Police probe Kabul suicide attack DESCRIPTION: Afghan police are investigating a suicide grenade attack in the center of of Kabul that injured seven people, including three international peacekeepers.\n",
            "TITLE: Google prepares to wrap up share auction (AFP) DESCRIPTION: AFP - Google Inc prepared to wrap up an extraordinary share auction, meaning a final price in the multi-billion-dollar listing will be announced within days.\n",
            "TITLE: Emap halts French magazine slump DESCRIPTION: Media group Emap reports a modest rise in interim profits and says it has stemmed falling sales at its French TV listings magazines.\n",
            "TITLE: Suspect in Srebrenica massacre arrested DESCRIPTION: One of the most feared members of the Bosnian-Serb army, who was accused of decapitating between 80 and 100 Muslims in 1995 and of carrying out the \n",
            "TITLE: Serena Easily Wins First U.S. Open Match (AP) DESCRIPTION: AP - Dressed for a night on the town, Serena Williams was all business in her first match in 4 1/2 weeks. Showing little sign of her injury-induced layoff, the two-time U.S. Open champion advanced to the second round with ease, overwhelming Sandra Kleinova of the Czech Republic 6-1, 6-3 Monday night.\n",
            "TITLE: Sen. Corzine: Sudan Not Disarming Militias in Darfur DESCRIPTION: Description: Sen. Jon Corzine (D-NJ), who just returned from a trip to Sudan, says he saw little evidence that Sudan is trying to disarm militias carrying out attacks in Darfur, despite government claims.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions for fine-tuning a GPT2-style model"
      ],
      "metadata": {
        "id": "6CWpeId2d8VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the amount of trainable parameters in a model to console.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\"-----------------------------------------------------------------------\")\n",
        "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\")\n",
        "    print(\"-----------------------------------------------------------------------\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "def fine_tune(model, epochs=1, batch_size=8):\n",
        "    \"\"\"\n",
        "    Just a simple function for fine-tuning a model like gpt-2 on the dataset\n",
        "    defined above\n",
        "    \"\"\"\n",
        "    LEARNING_RATE = 1e-5\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"EPOCH: {epoch} \" + '=' * 20)\n",
        "        with tqdm(enumerate(loader), total=len(loader)) as progress_bar:\n",
        "            for idx, batch in progress_bar:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "                input_ids = inputs['input_ids'].to(device)\n",
        "\n",
        "                outputs = model(input_ids, labels=input_ids)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                # Backward pass and optimization\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                progress_bar.set_description(f\"Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "mdHvknzaeAwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disadvantage of fully fine-tuning a model\n",
        "\n",
        "Full fine-tuning of a LLM typically requires extensive resources. The following example illustrates this. Utilizing Google Colab's free tier, which provides access to a single T4 GPU, I attempted to fine-tune GPT2-Medium (~355 million parameters) but encountered significant memory constraints."
      ],
      "metadata": {
        "id": "809PA1-L22q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Model\n",
        "model_name = \"gpt2-medium\"\n",
        "full_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Print Params\n",
        "print_trainable_parameters(full_model)\n",
        "\n",
        "# Fine-tune model\n",
        "fine_tune(full_model, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "Ugnhye6K27Qy",
        "outputId": "7a774268-b214-4f73-c0a5-68a43a73a6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------------\n",
            "trainable params: 354823168 || all params: 354823168 || trainable%: 100.00\n",
            "-----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "EPOCH: 0 ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 2.8686:   1%|          | 25/2500 [00:19<31:26,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 468.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 57.06 MiB is free. Process 31228 has 14.69 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 765.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-aea1f774fddf>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Fine-tune model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfine_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-54bac09fbb0f>\u001b[0m in \u001b[0;36mfine_tune\u001b[0;34m(model, epochs, batch_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 468.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 57.06 MiB is free. Process 31228 has 14.69 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 765.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA Model\n",
        "Implementation of LoRA (Low-Rank Adaptation) for fine-tuning a pre-trained causal language model like GPT-2. It initializes a pre-trained model, defines a custom LoRA_Linear module, and replaces modules related to the attention mechanism in the model with LoRA_Linear. Afterwards the model is fine-tuned. The results show, that the LoRA model can be fine-tuned without any memory issues.\n",
        "\n",
        "This specific implementation of LoRA was inspired by https://github.com/tsmatz/finetune_llm_with_lora\n",
        "\n"
      ],
      "metadata": {
        "id": "N5AN6n13R0gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "class LoRA_Linear(nn.Module):\n",
        "    def __init__(self, weight, bias, lora_dim):\n",
        "        super(LoRA_Linear, self).__init__()\n",
        "\n",
        "        out, inp = weight.shape\n",
        "\n",
        "        # Set up linear layer with old weight and bias\n",
        "        if bias is None:\n",
        "            self.linear = nn.Linear(inp, out, bias=False)\n",
        "            self.linear.load_state_dict({\"weight\": weight})\n",
        "        else:\n",
        "            self.linear = nn.Linear(inp, out)\n",
        "            self.linear.load_state_dict({\"weight\": weight, \"bias\": bias})\n",
        "\n",
        "        # Set up new LoRA weights\n",
        "        self.lora_right = nn.Parameter(torch.zeros(inp, lora_dim))\n",
        "        nn.init.kaiming_uniform_(self.lora_right, a=math.sqrt(5))\n",
        "        self.lora_left = nn.Parameter(torch.zeros(lora_dim, out))\n",
        "\n",
        "    def forward(self, input):\n",
        "        frozen_output = self.linear(input)\n",
        "        LoRA_output = input @ self.lora_right @ self.lora_left\n",
        "        return frozen_output + LoRA_output\n",
        "\n",
        "\n",
        "lora_dim = 8\n",
        "\n",
        "# Gather target modules\n",
        "targets = [n for n, _ in lora_model.named_modules() if \"attn.c_attn\" in n]\n",
        "\n",
        "# replace each module with LoRA\n",
        "for name in targets:\n",
        "    name_struct = name.split(\".\")\n",
        "\n",
        "    module_list = [lora_model]\n",
        "    for struct in name_struct:\n",
        "        module_list.append(getattr(module_list[-1], struct))\n",
        "\n",
        "    # build LoRA layer\n",
        "    lora = LoRA_Linear(\n",
        "        weight = torch.transpose(module_list[-1].weight, 0, 1), # old weight\n",
        "        bias = module_list[-1].bias, # old bias\n",
        "        lora_dim = lora_dim # lora dimensionality\n",
        "        )\n",
        "\n",
        "    # set child of parent to new LoRA layer\n",
        "    module_list[-2].__setattr__(name_struct[-1], lora)\n",
        "\n",
        "# Freeze all non-LoRA params\n",
        "for n, p in lora_model.named_parameters():\n",
        "    p.requires_grad = \"lora_right\" in n or \"lora_left\" in n\n",
        "\n",
        "lora_model = lora_model.to(device)\n",
        "\n",
        "print_trainable_parameters(lora_model)\n",
        "fine_tune(lora_model, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axdcYH2cR5zl",
        "outputId": "16c1cca8-eac0-4602-95f9-01a0bce2ff33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------------\n",
            "trainable params: 786432 || all params: 355609600 || trainable%: 0.22\n",
            "-----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "EPOCH: 0 ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 2.3973: 100%|██████████| 2500/2500 [16:07<00:00,  2.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation functions"
      ],
      "metadata": {
        "id": "4uJRQzSj--hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_from_top(probs, n=5):\n",
        "    \"\"\"\n",
        "    Selects one token ID from the top n probable token IDs in a given\n",
        "    probability distribution.\n",
        "    \"\"\"\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)\n",
        "\n",
        "def generate_example(model, query):\n",
        "    \"\"\"\n",
        "    This function generates text by repeatedly predicting the next token\n",
        "    using a given model and a starting query.\n",
        "    \"\"\"\n",
        "    indicator = query\n",
        "\n",
        "    cur_ids = torch.tensor(tokenizer.encode(indicator)).unsqueeze(0).to(device)\n",
        "    for i in range(2000):\n",
        "        outputs = model(cur_ids, labels=cur_ids)\n",
        "        loss, logits = outputs[:2]\n",
        "        softmax_logits = torch.softmax(logits[0,-1], dim=0)\n",
        "\n",
        "        next_token_id = choose_from_top(softmax_logits.to('cpu').detach().numpy(), n=5)\n",
        "        cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1)\n",
        "\n",
        "        if next_token_id in tokenizer.encode('<|endoftext|>'):\n",
        "            break\n",
        "\n",
        "        print(tokenizer.decode([next_token_id]), end='')\n",
        "        if i % 20 == 0 and i != 0:\n",
        "          print(\"\\n\", end='')\n",
        "\n",
        "\n",
        "def evaluate_model(model):\n",
        "    s_1 = \"TITLE: Big News at University of Freiburg! DESCRIPTION:\"\n",
        "\n",
        "    print(\"Model Input: {}\".format(s_1))\n",
        "    print(\"\")\n",
        "    print(\"Model Completion: \")\n",
        "    generate_example(model, s_1)"
      ],
      "metadata": {
        "id": "MCyYqZqC_Awv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examle of the fine-tuned LoRA Model"
      ],
      "metadata": {
        "id": "XZKHYBbby9lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model.eval()\n",
        "evaluate_model(lora_model)"
      ],
      "metadata": {
        "id": "AN-anKCw8YaB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f490bc10-5fc2-4c92-d754-97283c759518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Input: TITLE: Big News at University of Freiburg! DESCRIPTION:\n",
            "\n",
            "Model Completion: \n",
            " The University of Freiburg has released a new version of its software to improve the speed of the system\n",
            ", which is designed to help students study for exams faster."
          ]
        }
      ]
    }
  ]
}